# Copyright (c) Pixie Labs, Inc.
# Licensed under the Apache License, Version 2.0 (the "License")

''' CQL Pod LET metrics

This live view calculates the latency, error rate, and throughput
of a pod's CQL requests.

'''
import px

# ----------------------------------------------------------------
# Visualization Variables - No need to edit for basic configuration.
# ----------------------------------------------------------------
# K8s object is the abstraction to group on.
# Options are ['pod', 'service'].
k8s_object = 'pod'
ns_per_ms = 1000 * 1000
ns_per_s = 1000 * ns_per_ms
# Window size to use on time_ column for bucketing.
window_ns = px.DurationNanos(10 * ns_per_s)
# Column name used to split data into separate time series.
# k8s_object column is renamed to this and is used in
# visualization spec.
split_series_name = 'k8s'
# This is the name of the column corresponding to the CQL IP.
ip_col_name = 'CQL IP'
# Temporary way to ensure px.Pod works as expected.
px.Pod = str
# The bin size to use for the latency histogram.
latency_bin_size_ns = px.DurationNanos(50 * ns_per_ms)
# ----------------------------------------------------------------


# ----------------------------------------------------------------
# Visualization functions:
#
# These functions are formatted and ready for use in
# the visualization speciciation, vis.json.
# ----------------------------------------------------------------
def pod_cql_let(start: str, pod: px.Pod):
    """ Calculate LET time-series for CQL traffic per Pod, database pair.

    Calculates latency, error_rate, and throughput for each pod's
    connection to CQL databases.

    @start: The timestamp of data to start at.
    @pod: the partial/full-name of the pod to monitor CQL LET.

    Returns: Returns the DataFrame contiaining LET time-series for CQL
        traffic for each database Pods talk to.
    """
    df = cql_let_per_pod(start, pod)
    return df['time_', split_series_name, ip_col_name, 'latency_p50',
              'latency_p90', 'latency_p99', 'error_rate', 'request_throughput']


def summary_cql_let(start: str, pod: px.Pod):
    """ Calculate LET summary for CQL traffic per Pod, database pair.

    Calculates latency, error_rate, and throughput for each pod's
    connection to CQL databases.

    @start: The timestamp of data to start at.
    @pod: the partial/full-name of the pod to monitor CQL LET.

    Returns: Returns the DataFrame contiaining LET time-series for CQL
        traffic for each database Pods talk to.
    """
    df = cql_let_per_pod(start, pod)
    summary_df = summarize_LET(df, [k8s_object, ip_col_name])
    return summary_df[[k8s_object, ip_col_name, "request_throughput",
                       "error_rate", "latency", "total_requests"]]


def latency_histogram(start: str, pod: px.Pod):
    """ Computes a histogram of HTTP request latency.

    Args:
    @start: The timestamp of data to start at.
    @svc: the partial/full-name of the svc.

    Returns: DataFrame of the HTTP latency histogram for svcs that
        match @svc.
    """
    # The data necessary to compute CQL LET information is located in the
    # cql_events table. We filter and aggregate data from this table to compute the
    # required metrics.
    df = px.DataFrame(table='cql_events', start_time=start)
    df = format_cql_table(df)

    # Calculate LET of pod(s) (k8s_object) connection to CQL conections
    # over the time window ('timestamp') after filtering for matching svcs.
    matching_df = df[px.contains(df[k8s_object], pod)]

    matching_df.request_latency = px.bin(matching_df.latency,
                                         latency_bin_size_ns)
    return matching_df.groupby('request_latency').agg(count=('time_', px.count))


# ----------------------------------------------------------------
# Utility functions:
#
# These are shared functions. We plan to support imports in v0.3,
# which will allow these functions to be shared across multiple
# scripts.
# ----------------------------------------------------------------
def cql_let_per_pod(start: str, pod: px.Pod):
    """ Calculate LET time-series for CQL traffic per Pod, database pair.

    Calculates latency, error_rate, and throughput for each pod's
    connection to CQL databases.

    @start: The timestamp of data to start at.
    @pod: the partial/full-name of the pod to monitor CQL LET.

    Returns: Returns the DataFrame contiaining LET time-series for CQL
        traffic for each database Pods talk to.
    """
    # The data necessary to compute CQL LET information is located in the
    # cql_events table. We filter and aggregate data from this table to compute the
    # required metrics.
    df = px.DataFrame(table='cql_events', start_time=start)
    df = format_cql_table(df)

    # Calculate LET of pod(s) (k8s_object) connection to CQL conections
    # over the time window ('timestamp') after filtering for matching svcs.
    matching_df = df[px.contains(df[k8s_object], pod)]
    let_df = calc_cql_LET(
        matching_df, [k8s_object, 'timestamp', 'remote_addr'])
    let_df[split_series_name] = let_df[k8s_object]
    let_df[ip_col_name] = let_df.remote_addr
    return let_df


def format_events_table(df, latency_col):
    """ Format data and add semantic columns in event tables

    Unifies latency column to 'latency_ms', adds a binned
    timestamp field to aggregate on, and adds the svc
    (k8s_object) as a semantic column.

    Works on "cql_events" and "http_events"

    Args:
    @df: the input events table
    @latency_col: the name of the latency column in @df.

    Returns: formatted events DataFrame
    """
    df.latency = df[latency_col]

    df.timestamp = px.bin(df.time_, window_ns)
    df[k8s_object] = df.ctx[k8s_object]
    df = df[df[k8s_object] != '']
    return df


def format_cql_table(df):
    """ Formats cql_events tables

    Runs events table universal formatting, creates a failure field
    marking which requests receive an error status code.

    Args:
    @df: the input cql_events table.

    Returns: formatted cql_events DataFrame.
    """
    df = format_events_table(df, 'latency')
    df.failure = df['resp_op'] == 0
    df.resp_size = px.length(df.resp_body)

    return df


def format_LET_aggs(df):
    """ Converts the result of LET windowed aggregates into expected metrics.

    Converts the result of aggregates on windows into well-formatted metrics that
    can be visualized. Latency quantile values need to be extracted from the
    quantiles struct, and then error_rate, request_throughput, and bytes_per_ns are calculated as
    a function of window size.


    This function represents logic shared by LET calculators for CQL and
    HTTP events.

    Args:
    @df: the input events table grouped into windows with aggregated
        columns 'throughput_total', 'error_rate_per_window', and 'request_throughput'

    Returns: DataFrame with formatted LET metrics.
    """

    df.latency_p50 = px.pluck_float64(df.latency_quantiles, 'p50')
    df.latency_p90 = px.pluck_float64(df.latency_quantiles, 'p90')
    df.latency_p99 = px.pluck_float64(df.latency_quantiles, 'p99')
    df['time_'] = df['timestamp']
    df.request_throughput = df.throughput_total / window_ns
    df.bytes_per_ns = df.bytes_total / window_ns
    df.error_rate = df.error_rate_per_window * df.request_throughput / px.DurationNanos(1)

    return df


def calc_cql_LET(df, groups):
    """ Calculates Latency, Error Rate, and Throughput on CQL events.

    Calculates latency, error rate, and throughput aggregated over
    @groups.

    Args:
    @df: the input cql_events table.
    @groups: the list of columns to group on. 'timestamp' must be a a group
        or this will fail.

    Returns: The LET DataFrame.
    """
    # All requests for errors and throughput
    et = df.groupby(groups).agg(
        throughput_total=('latency', px.count),
        error_rate_per_window=('failure', px.mean),
    )

    filt_df = df[df.resp_op != 0]
    # Calculate latency on all requests that return or return an err.
    lcy = filt_df.groupby(groups).agg(
        latency_quantiles=('latency', px.quantiles),
        bytes_total=('resp_size', px.sum)
    )
    # Left join because et's groups are a strict superset of lcy.
    df = et.merge(lcy, how='left', left_on=groups,
                  right_on=groups, suffixes=['', '_x'])

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df = format_LET_aggs(df)
    return df


def ip_to_svc_name(df, ip_col, svc_col_name):
    """ Map IP to svc name.

    Maps IP values stored in @ip_col into svc names to store into
    @svc_col_name.

    Args:
    @df: the input dataframe.
    @ip_col: the IP column to map from.
    @svc_col_name: the column name to assign the new svc values.

    Returns: DataFrame with the svc_col added.
    """
    pod_id = 'pod_id'
    df[pod_id] = px.ip_to_pod_id(df[ip_col])
    df[svc_col_name] = px.pod_id_to_service_name(df[pod_id])
    return df.drop(pod_id)


def summarize_LET(let_df, groups):
    """ Aggregate LET values across all windows.

    Args:
    @let_df: the DataFrame with LET values.
    @groups: the columns to group over.

    Returns: The summary DF.
    """
    df = let_df.groupby(groups).agg(
        request_throughput=('request_throughput', px.mean),
        bytes_per_ns=('bytes_per_ns', px.mean),
        error_rate=('error_rate', px.mean),
        total_requests=('throughput_total', px.sum),
        latency=('latency_p50', px.mean),
    )
    return df
