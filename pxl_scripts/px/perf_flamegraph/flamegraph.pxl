# Copyright (c) Pixie Labs, Inc.
# Licensed under the Apache License, Version 2.0 (the "License")

'''CPU Flamegraph

Performance profile visualization.

'''

import px


def stacktraces(start_time: str, node: str, namespace: str, pod: str, pct_basis_entity: str):
    df = px.DataFrame(table='stack_traces.beta', start_time=start_time)

    df.namespace = df.ctx['namespace']
    df.pod = df.ctx['pod']
    df.container = df.ctx['container']
    df.cmdline = df.ctx['cmdline']

    # Compute node using _exec_hostname() instead of `df.ctx['node']`
    # We do this so it works for non-k8s processes too.
    # This is important for determining total number of stack trace samples per node,
    # as we need to include the non-K8s processes in the computation.
    df.node = px.Node(px._exec_hostname())

    # This variable is used when later computing percentages.
    # When pct_basis_entity is pod, then the computation is simple:
    #    percentage = num_samples_symbol / num_samples_pod
    # But when pct_basis_entity is node, the computation needs to account for the number of CPUs,
    # because we want CPU percentages, not Node percentages (a Node may have multiple CPUs).
    # The formula then becomes:
    #    percentage = num_samples_symbol / (num_samples_node / num_cpus )
    # Note that the difference between the two formulas is that the denominator is scaled.
    # We call this extra quotient the "scaling_factor".
    # Also note each node may have a different number of CPUs, so we have to compute this early.
    df.scaling_factor = px.select(pct_basis_entity == 'node', px._exec_host_num_cpus(), 1)

    # This must be done before any filtering, to get accurate stack trace totals.
    grouping_agg = df.groupby([pct_basis_entity]).agg(
        count=('count', px.sum),
        scaling_factor=('scaling_factor', px.any)
    )

    # Apply filters.
    df = df[px.contains(df.node, node)]
    df = df[px.contains(df.pod, pod)]
    df = df[px.contains(df.namespace, namespace)]

    # Filter out any non-k8s processes (it's now safe to do so).
    df = df[df.pod != '']

    # Aggregate stack-traces from different profiles into one larger profile.
    # For example, if a profile is generated every 30 seconds, and our query spans 5 minutes,
    # this merges the 10 profiles into a single profile including samples for entire 5 minutes.
    df = df.groupby(['node', 'namespace', 'pod', 'container', 'cmdline', 'stack_trace_id']).agg(
        stack_trace=('stack_trace', px.any),
        count=('count', px.sum)
    )

    # Compute percentages.
    df = df.merge(
        grouping_agg,
        how='inner',
        left_on=pct_basis_entity,
        right_on=pct_basis_entity,
        suffixes=['', '_x']
    )
    df.percent = 100.0 * df.count * df.scaling_factor / df.count_x
    df.drop(pct_basis_entity + '_x')

    return df
