# Copyright (c) Pixie Labs, Inc.
# Licensed under the Apache License, Version 2.0 (the "License")

''' Endpoint Overview

 This script gets an overview of an individual endpoint on an individual service, summarizing its request statistics.
'''

import px

ns_per_ms = 1000 * 1000
ns_per_s = 1000 * ns_per_ms
# Window size to use on time_ column for bucketing.
window_ns = px.DurationNanos(10 * ns_per_s)
# Flag to filter out requests that come from an unresolvable IP.
filter_unresolved_inbound = False
# Flag to filter out health checks from the data.
filter_health_checks = False
# Flag to filter out ready checks from the data.
filter_ready_checks = False


def endpoint_let_timeseries(start_time: str, service: px.Service, endpoint: str):
    ''' Compute the let as a timeseries for requests received by `service` at `endpoint`.

    Args:
    @start_time: The timestamp of data to start at.
    @service: The name of the service to filter on.
    @endpoint: The endpoint to filter on.

    '''
    df = let_helper(start_time, service)
    df = df[px._match_endpoint(df.http_req_path, endpoint)]

    df = df.groupby(['timestamp']).agg(
        latency_quantiles=('latency', px.quantiles),
        error_rate_per_window=('failure', px.mean),
        throughput_total=('latency', px.count),
        bytes_total=('resp_size', px.sum)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df.latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p99')))
    df.request_throughput = df.throughput_total / window_ns
    df.errors_per_ns = df.error_rate_per_window * df.request_throughput / px.DurationNanos(1)
    df.error_rate = px.Percent(df.error_rate_per_window)
    df.bytes_per_ns = df.bytes_total / window_ns
    df.time_ = df.timestamp

    return df[['time_', 'latency_p50', 'latency_p90', 'latency_p99',
               'request_throughput', 'errors_per_ns', 'error_rate', 'bytes_per_ns']]


def endpoint_slow_requests(start_time: str, service: px.Service, endpoint: str):
    df = let_helper(start_time, service)
    df = df[px._match_endpoint(df.http_req_path, endpoint)]
    quantiles = df.groupby('service').agg(
        latency_quantiles=('latency', px.quantiles)
    )
    quantiles.service_p99 = px.pluck_float64(quantiles.latency_quantiles, 'p99')
    quantiles = quantiles.drop('latency_quantiles')
    requests = df.merge(quantiles, left_on='service', right_on='service', how='inner',
                        suffixes=['', '_x'])
    requests = requests[requests.latency >= px.floor(requests.service_p99)]
    return requests[['time_', 'pod', 'latency', 'http_req_method',
                     'http_req_path', 'http_req_body', 'http_resp_status',
                     'remote_addr', 'remote_port',
                     'http_resp_message', 'http_resp_body']].head(100)


def let_helper(start_time: str, service: px.Service):
    ''' Compute the initial part of the let for requests.

    Args:
    @start_time: The timestamp of data to start at.
    @service: The service to filter on.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time)
    df.service = df.ctx['service']
    df = df[px.contains(df.service, service)]
    df.pod = df.ctx['pod']
    df.latency = df.http_resp_latency_ns

    df.timestamp = px.bin(df.time_, window_ns)

    df.resp_size = px.Bytes(px.length(df.http_resp_body))
    df.failure = df.http_resp_status >= 400
    filter_out_conds = ((df.http_req_path != '/health' or not filter_health_checks) and (
        df.http_req_path != '/readyz' or not filter_ready_checks)) and (
        df['remote_addr'] != '-' or not filter_unresolved_inbound)

    df = df[filter_out_conds]
    return df
