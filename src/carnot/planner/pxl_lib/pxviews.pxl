R"(
# Copyright 2018- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0


import px


def pod_resource_stats(start_time, end_time):
    ''' Resource usage statistics for each pod.
    Provides start_time, number of containers, associated node, cpu, disk I/O and memory info.
    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    '''
    pod_metadata_df = px.DataFrame(table='process_stats', start_time=start_time, end_time=end_time)
    pod_metadata_df.pod = pod_metadata_df.ctx['pod_name']
    pod_metadata_df.container = pod_metadata_df.ctx['container_id']
    pod_metadata_df = pod_metadata_df.groupby(['pod', 'container']).agg()
    pod_metadata_df = pod_metadata_df.groupby(['pod']).agg(container_count=('container', px.count))

    df = px.DataFrame(table='process_stats', start_time=start_time, end_time=end_time)
    df['pod_id'] = df.ctx['pod_id']
    df = df.groupby(['pod_id', 'upid']).agg(
        rss=('rss_bytes', px.mean),
        vsize=('vsize_bytes', px.mean),
        # The fields below are counters, so we take the min and the max to subtract them.
        cpu_utime_ns_max=('cpu_utime_ns', px.max),
        cpu_utime_ns_min=('cpu_utime_ns', px.min),
        cpu_ktime_ns_max=('cpu_ktime_ns', px.max),
        cpu_ktime_ns_min=('cpu_ktime_ns', px.min),
        read_bytes_max=('read_bytes', px.max),
        read_bytes_min=('read_bytes', px.min),
        write_bytes_max=('write_bytes', px.max),
        write_bytes_min=('write_bytes', px.min),
        rchar_bytes_max=('rchar_bytes', px.max),
        rchar_bytes_min=('rchar_bytes', px.min),
        wchar_bytes_max=('wchar_bytes', px.max),
        wchar_bytes_min=('wchar_bytes', px.min),
        time_max=('time_', px.max),
        time_min=('time_', px.min),
    )

    df.timestamp = df.time_max
    # Convert counters into gauges by subtracting the value at the start of the window from the end of the window.
    df.cpu_utime_ns = df.cpu_utime_ns_max - df.cpu_utime_ns_min
    df.cpu_ktime_ns = df.cpu_ktime_ns_max - df.cpu_ktime_ns_min
    df.read_bytes = df.read_bytes_max - df.read_bytes_min
    df.write_bytes = df.write_bytes_max - df.write_bytes_min
    df.rchar_bytes = df.rchar_bytes_max - df.rchar_bytes_min
    df.wchar_bytes = df.wchar_bytes_max - df.wchar_bytes_min
    df.window = df.time_max - df.time_min

    # Sum up the UPID values.
    df = df.groupby(['pod_id']).agg(
        cpu_ktime_ns=('cpu_ktime_ns', px.sum),
        cpu_utime_ns=('cpu_utime_ns', px.sum),
        read_bytes=('read_bytes', px.sum),
        write_bytes=('write_bytes', px.sum),
        rchar_bytes=('rchar_bytes', px.sum),
        wchar_bytes=('wchar_bytes', px.sum),
        rss=('rss', px.sum),
        vsize=('vsize', px.sum),
        # We take the max across all windows for a pod as a best effort to
        # show the rate of resource usage for the pod. That means dead pods that
        # used a large amount of resources during their lifetimes will appear
        # as if they are still using the large resources if the window includes
        # their data.
        window=('window', px.max),
        timestamp=('timestamp', px.max),
    )

    # If the window size is 0, we set window to 1 to avoid NaNs.
    # the rates will be calculated to 0 because a zero window size
    # will mean that min == max above for all counters.
    df.window = px.select(df.window > 0, df.window, 1)
    # Divide the values by the size of the window to get the rate.
    df.actual_disk_read_throughput = df.read_bytes / df.window
    df.actual_disk_write_throughput = df.write_bytes / df.window
    df.total_disk_read_throughput = df.rchar_bytes / df.window
    df.total_disk_write_throughput = df.wchar_bytes / df.window

    # Sum the cpu_usage into one value.
    df.cpu_usage = px.Percent((df.cpu_ktime_ns + df.cpu_utime_ns) / df.window)
    df.pod = df.ctx['pod']
    df = df.drop(['window', 'cpu_ktime_ns', 'cpu_utime_ns', 'read_bytes', 'rchar_bytes', 'write_bytes', 'wchar_bytes'])
    df = df.merge(pod_metadata_df, how='inner', left_on='pod', right_on='pod',
                  suffixes=['', '_x']).drop(['pod_x'])

    df.service = df.ctx['service_name']
    df.node = df.ctx['node_name']
    df.namespace = df.ctx['namespace']
    df.start_time = px.pod_name_to_start_time(df.pod)
    df.status = px.pod_name_to_status(df.pod)
    return df[['pod_id', 'pod', 'service', 'node', 'namespace', 'container_count',
               'start_time', 'status', 'cpu_usage', 'rss', 'vsize',
               'actual_disk_read_throughput', 'actual_disk_write_throughput',
               'total_disk_read_throughput', 'total_disk_write_throughput']]


def _http_events(start_time, end_time, include_health_checks, include_ready_checks, include_unresolved_inbound):
    ''' Returns a dataframe of http events with health/ready requests filtered out and a
        failure field added.

    Note this script is prefixed with an underscore and therefore at risk of changing in the future.
    Rely on this function at your own risk.

    Args:
    @start_time: The timestamp of data to start at.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time, end_time=end_time)
    df.failure = df.resp_status >= 400

    filter_out_conds = ((df.req_path != '/healthz' or include_health_checks) and (
        df.req_path != '/readyz' or include_ready_checks)) and (
        df['remote_addr'] != '-' or include_unresolved_inbound)
    df = df[filter_out_conds]

    return df


def inbound_http_summary(start_time, end_time):
    ''' Gets a summary of requests inbound to `pod`.

    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    '''
    df = _http_events(start_time,
                     end_time,
                     include_health_checks=False,
                     include_ready_checks=False,
                     include_unresolved_inbound=False)
    df.pod_id = df.ctx['pod_id']

    # Filter only to inbound pod traffic (server-side).
    # Don't include traffic initiated by this pod to an external location.
    df = df[df.trace_role == 2]

    df = df.groupby(['pod_id', 'remote_addr']).agg(
        latency=('latency', px.quantiles),
        num_requests=('time_', px.count),
        stop_time=('time_', px.max),
        num_errors=('failure', px.sum),

        # TODO(philkuz) calculate this from the conn_stats table.
        inbound_bytes=('req_body_size', px.sum),
        outbound_bytes=('resp_body_size', px.sum)
    )

    df.requesting_ip = df.remote_addr
    df.requesting_pod_id = px.ip_to_pod_id(df.remote_addr)
    df.requesting_pod = px.pod_id_to_pod_name(df.requesting_pod_id)
    df.requesting_svc = px.pod_id_to_service_name(df.requesting_pod_id)
    df.time_ = df.stop_time

    return df[['time_', 'pod_id', 'requesting_ip', 'requesting_pod', 'requesting_svc', 'latency',
                         'num_errors', 'num_requests', 'inbound_bytes', 'outbound_bytes']]


def http_graph(start_time, end_time):
    ''' Calculates the graph of all http requests made within a cluster.

    Calculates a graph of all the pods that make http requests to each other. Users
    can filter the graph per namespace, service, pod, or ip.

    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    '''
    # TODO(philkuz) fix pxviews can't use False because it's not in scope.
    df = _http_events(start_time,
                     end_time,
                     include_health_checks=False,
                     include_ready_checks=False,
                     include_unresolved_inbound=False)
    df.pod_id = df.ctx['pod_id']
    df = df.groupby(['pod_id', 'remote_addr', 'trace_role']).agg(
        latency=('latency', px.quantiles),
        total_request_count=('latency', px.count)
        num_requests=('time_', px.count),
        stop_time=('time_', px.max),
        num_errors=('failure', px.sum),
        inbound_bytes=('req_body_size', px.sum),
        outbound_bytes=('resp_body_size', px.sum)
    )

    df.traced_pod = df.ctx['pod']
    df.traced_ip = px.pod_name_to_pod_ip(df.traced_pod)
    df.traced_service = df.ctx['service']

    # Get the traced and remote pod/service/IP information.
    df.remote_ip = df.remote_addr
    localhost_ip_regexp = r'127\.0\.0\.[0-9]+'
    df.is_remote_addr_localhost = px.regex_match(localhost_ip_regexp, df.remote_ip)
    df.remote_pod = px.select(df.is_remote_addr_localhost,
                              'localhost:' + df.traced_pod,
                              px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_ip)))
    df.remote_service = px.select(df.is_remote_addr_localhost,
                                  'localhost:' + df.traced_service,
                                  px.service_id_to_service_name(px.ip_to_service_id(df.remote_ip)))

    # Assign requestor and responder based on the trace_role.
    df.is_server_side_tracing = df.trace_role == 2
    df.responder_ip = px.select(df.is_server_side_tracing, df.traced_ip, df.remote_ip)
    df.requestor_ip = px.select(df.is_server_side_tracing, df.remote_ip, df.traced_ip)

    df.responder_pod = px.select(df.is_server_side_tracing, df.traced_pod, df.remote_pod)
    df.requestor_pod = px.select(df.is_server_side_tracing, df.remote_pod, df.traced_pod)

    df.responder_service = px.select(df.is_server_side_tracing, df.traced_service, df.remote_service)
    df.requestor_service = px.select(df.is_server_side_tracing, df.remote_service, df.traced_service)

    df.latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.latency, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.latency, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.latency, 'p99')))

    df = df.groupby([
        'responder_pod',
        'requestor_pod',
        'responder_ip',
        'requestor_ip',
        'responder_service',
        'requestor_service'
    ]).agg(
        # TODO(philkuz) build a combine_quantiles udf that does this properly.
        # latency=('latency', px.combine_quantiles),
        latency_p50=('latency_p50', px.mean),
        latency_p90=('latency_p90', px.mean),
        latency_p99=('latency_p99', px.mean),
        num_requests=('num_requests', px.sum),
        num_errors=('num_errors', px.sum),
        inbound_bytes=('inbound_bytes', px.sum),
        outbound_bytes=('outbound_bytes', px.sum),
        stop_time=('stop_time', px.max),
    )

    df.time_ = df.stop_time
    return df[['time_', 'responder_pod', 'requestor_pod',
               'responder_ip', 'requestor_ip','responder_service', 'requestor_service',
               'latency_p50', 'latency_p90', 'latency_p99', 'num_requests',
               'num_errors', 'inbound_bytes', 'outbound_bytes']]


def container_process_timeseries(start_time, end_time, window_ns):
    ''' Compute the timeseries of CPU, memory, and IO usage for each container
    sourced from the `process_stats` table.

    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    @window_ns The window size in nanoseconds for the timeseries.
    '''
    df = px.DataFrame(table='process_stats', start_time=start_time, end_time=end_time)
    df.timestamp = px.bin(df.time_, window_ns)
    df.container = df.ctx['container_name']
    df.pod_id = df.ctx['pod_id']

    # First calculate CPU usage by process (UPID) in each k8s_object
    # over all windows.
    df = df.groupby(['upid', 'container','pod_id', 'timestamp']).agg(
        rss=('rss_bytes', px.mean),
        vsize=('vsize_bytes', px.mean),
        # The fields below are counters, so we take the min and the max to subtract them.
        cpu_utime_ns_max=('cpu_utime_ns', px.max),
        cpu_utime_ns_min=('cpu_utime_ns', px.min),
        cpu_ktime_ns_max=('cpu_ktime_ns', px.max),
        cpu_ktime_ns_min=('cpu_ktime_ns', px.min),
        read_bytes_max=('read_bytes', px.max),
        read_bytes_min=('read_bytes', px.min),
        write_bytes_max=('write_bytes', px.max),
        write_bytes_min=('write_bytes', px.min),
        rchar_bytes_max=('rchar_bytes', px.max),
        rchar_bytes_min=('rchar_bytes', px.min),
        wchar_bytes_max=('wchar_bytes', px.max),
        wchar_bytes_min=('wchar_bytes', px.min),
    )

    # Next calculate cpu usage and memory stats per window.
    df.cpu_utime_ns = df.cpu_utime_ns_max - df.cpu_utime_ns_min
    df.cpu_ktime_ns = df.cpu_ktime_ns_max - df.cpu_ktime_ns_min
    df.actual_disk_read_throughput = (df.read_bytes_max - df.read_bytes_min) / window_ns
    df.actual_disk_write_throughput = (df.write_bytes_max - df.write_bytes_min) / window_ns
    df.total_disk_read_throughput = (df.rchar_bytes_max - df.rchar_bytes_min) / window_ns
    df.total_disk_write_throughput = (df.wchar_bytes_max - df.wchar_bytes_min) / window_ns


    # Then aggregate per container.
    df = df.groupby(['timestamp', 'pod_id', 'container']).agg(
        cpu_ktime_ns=('cpu_ktime_ns', px.sum),
        cpu_utime_ns=('cpu_utime_ns', px.sum),
        actual_disk_read_throughput=('actual_disk_read_throughput', px.sum),
        actual_disk_write_throughput=('actual_disk_write_throughput', px.sum),
        total_disk_read_throughput=('total_disk_read_throughput', px.sum),
        total_disk_write_throughput=('total_disk_write_throughput', px.sum),
        rss=('rss', px.sum),
        vsize=('vsize', px.sum),
    )

    # Finally, calculate total (kernel + user time)  percentage used over window.
    df.cpu_usage = px.Percent((df.cpu_ktime_ns + df.cpu_utime_ns) / window_ns)
    df.time_ = df.timestamp
    return df[['time_', 'pod_id', 'container', 'cpu_usage', 'actual_disk_read_throughput',
            'actual_disk_write_throughput', 'total_disk_read_throughput',
            'total_disk_write_throughput', 'rss', 'vsize']]


def pod_network_timeseries(start_time, end_time, window_ns):
    ''' Compute the timeseries of network events for each pod.

    Returns timeseries data summarizing the bytes, drops, and errors for
    the incoming (rx) and outgoing(tx) network connections for each pod.

    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    @window_ns The window size in nanoseconds for the timeseries.
    '''
    df = px.DataFrame(table='network_stats', start_time=start_time, end_time=end_time)
    df.timestamp = px.bin(df.time_, window_ns)

    # First calculate network usage by node over all windows.
    # Data is sharded by Pod in network_stats.
    df = df.groupby(['timestamp', 'pod_id']).agg(
        rx_bytes_end=('rx_bytes', px.max),
        rx_bytes_start=('rx_bytes', px.min),
        tx_bytes_end=('tx_bytes', px.max),
        tx_bytes_start=('tx_bytes', px.min),
        tx_errors_end=('tx_errors', px.max),
        tx_errors_start=('tx_errors', px.min),
        rx_errors_end=('rx_errors', px.max),
        rx_errors_start=('rx_errors', px.min),
        tx_drops_end=('tx_drops', px.max),
        tx_drops_start=('tx_drops', px.min),
        rx_drops_end=('rx_drops', px.max),
        rx_drops_start=('rx_drops', px.min),
    )

    # Calculate the network statistics rate over the window.
    # We subtract the counter value at the beginning ('_start')
    # from the value at the end ('_end').
    df.rx_bytes_per_ns = (df.rx_bytes_end - df.rx_bytes_start) / window_ns
    df.tx_bytes_per_ns = (df.tx_bytes_end - df.tx_bytes_start) / window_ns
    df.rx_drops_per_ns = (df.rx_drops_end - df.rx_drops_start) / window_ns
    df.tx_drops_per_ns = (df.tx_drops_end - df.tx_drops_start) / window_ns
    df.rx_errors_per_ns = (df.rx_errors_end - df.rx_errors_start) / window_ns
    df.tx_errors_per_ns = (df.tx_errors_end - df.tx_errors_start) / window_ns

    df.time_ = df.timestamp
    return df[['time_', 'pod_id', 'rx_bytes_per_ns', 'tx_bytes_per_ns',
               'rx_drops_per_ns', 'tx_drops_per_ns', 'rx_errors_per_ns', 'tx_errors_per_ns']]


def inbound_http_throughput_timeseries(start_time, end_time, window_ns):
    ''' Compute the timeseries statistics of inbound HTTP requests for each container
    in the cluster.

    Returns the portion of erroroneous requests and the total number of requests
    for each container, labelled by pod.

    Args:
    @start_time Starting time of the data to examine.
    @end_time Ending time of the data to examine.
    @window_ns The window size in nanoseconds for the timeseries.
    '''
    df = _http_events(start_time, end_time,
                      include_health_checks=False,
                      include_ready_checks=False,
                      include_unresolved_inbound=False)

    # Filter only to inbound pod traffic (server-side).
    # Don't include traffic initiated by this pod to an external location.
    df = df[df.trace_role == 2]

    df.pod_id = df.ctx['pod_id']
    df.container = df.ctx['container']
    df.timestamp = px.bin(df.time_, window_ns)
    df = df.groupby(['timestamp', 'container', 'pod_id']).agg(
        num_requests=('latency', px.count),
        num_errors=('failure', px.sum),
    )
    df.time_ = df.timestamp

    return df[['time_', 'pod_id', 'container', 'num_errors', 'num_requests']]

)"
