# Copyright 2018- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

'''Pod Overview

Overview of a specific Pod monitored by Pixie with its high level application metrics
(latency, error-rate & rps) and resource usage (cpu, writes, reads).

'''
import px

ns_per_ms = 1000 * 1000
ns_per_s = 1000 * ns_per_ms
# Window size to use on time_ column for bucketing.
window_ns = px.DurationNanos(10 * ns_per_s)
# Flag to filter out requests that come from an unresolvable IP.
filter_unresolved_inbound = True
# Flag to filter out health checks from the data.
filter_health_checks = True
# Flag to filter out ready checks from the data.
filter_ready_checks = True


def containers(start_time: str, pod: px.Pod):
    ''' A list of containers in `pod`.

    Args:
    @start_time: The timestamp of data to start at.
    @pod: The name of the pod to filter on.

    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df = df[df.ctx['pod'] == pod]
    df.name = df.ctx['container_name']
    df.id = df.ctx['container_id']
    df = df.groupby(['name', 'id']).agg()
    df.status = px.container_id_to_status(df.id)
    return df


def node(start_time: str, pod: px.Pod):
    ''' A list containing the node the `pod` is running on.

    Args:
    @start_time: The timestamp of data to start at.
    @pod: The name of the pod to filter on.

    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df = df[df.ctx['pod'] == pod]
    df.node = df.ctx['node_name']
    df.service = df.ctx['service']
    df.pod_id = df.ctx['pod_id']
    df.pod_name = df.ctx['pod']
    df = df.groupby(['node', 'service', 'pod_id', 'pod_name']).agg()
    df.pod_start_time = px.pod_name_to_start_time(df.pod_name)
    df.status = px.pod_name_to_status(df.pod_name)
    return df.drop('pod_name')


def processes(start_time: str, pod: px.Pod):
    ''' A list of processes in `pod`.

    Args:
    @start_time: The timestamp of data to start at.
    @pod: The name of the pod to filter on.

    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df = df[df.ctx['pod'] == pod]
    df.cmd = df.ctx['cmdline']
    df.pid = df.ctx['pid']
    df = df.groupby(['pid', 'cmd', 'upid']).agg()
    return df


def resource_timeseries(start_time: str, pod: px.Pod):
    ''' Compute the resource usage as a timeseries for `pod`.

    Args:
    @start_time: The timestamp of data to start at.
    @pod: The name of the pod to filter on.

    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df = df[df.ctx['pod'] == pod]
    df.timestamp = px.bin(df.time_, window_ns)
    df.container = df.ctx['container_name']

    # First calculate CPU usage by process (UPID) in each k8s_object
    # over all windows.
    df = df.groupby(['upid', 'container', 'timestamp']).agg(
        rss=('rss_bytes', px.mean),
        vsize=('vsize_bytes', px.mean),
        # The fields below are counters, so we take the min and the max to subtract them.
        cpu_utime_ns_max=('cpu_utime_ns', px.max),
        cpu_utime_ns_min=('cpu_utime_ns', px.min),
        cpu_ktime_ns_max=('cpu_ktime_ns', px.max),
        cpu_ktime_ns_min=('cpu_ktime_ns', px.min),
        read_bytes_max=('read_bytes', px.max),
        read_bytes_min=('read_bytes', px.min),
        write_bytes_max=('write_bytes', px.max),
        write_bytes_min=('write_bytes', px.min),
        rchar_bytes_max=('rchar_bytes', px.max),
        rchar_bytes_min=('rchar_bytes', px.min),
        wchar_bytes_max=('wchar_bytes', px.max),
        wchar_bytes_min=('wchar_bytes', px.min),
    )


    # Next calculate cpu usage and memory stats per window.
    df.cpu_utime_ns = df.cpu_utime_ns_max - df.cpu_utime_ns_min
    df.cpu_ktime_ns = df.cpu_ktime_ns_max - df.cpu_ktime_ns_min
    df.actual_disk_read_throughput = (df.read_bytes_max - df.read_bytes_min) / window_ns
    df.actual_disk_write_throughput = (df.write_bytes_max - df.write_bytes_min) / window_ns
    df.total_disk_read_throughput = (df.rchar_bytes_max - df.rchar_bytes_min) / window_ns
    df.total_disk_write_throughput = (df.wchar_bytes_max - df.wchar_bytes_min) / window_ns

    # Then aggregate process individual process metrics.
    df = df.groupby(['timestamp', 'container']).agg(
        cpu_ktime_ns=('cpu_ktime_ns', px.sum),
        cpu_utime_ns=('cpu_utime_ns', px.sum),
        actual_disk_read_throughput=('actual_disk_read_throughput', px.sum),
        actual_disk_write_throughput=('actual_disk_write_throughput', px.sum),
        total_disk_read_throughput=('total_disk_read_throughput', px.sum),
        total_disk_write_throughput=('total_disk_write_throughput', px.sum),
        rss=('rss', px.sum),
        vsize=('vsize', px.sum),
    )

    # Finally, calculate total (kernel + user time)  percentage used over window.
    df.cpu_usage = px.Percent((df.cpu_ktime_ns + df.cpu_utime_ns) / window_ns)
    df.time_ = df.timestamp
    return df.drop(['cpu_ktime_ns', 'cpu_utime_ns', 'timestamp'])


def network_timeseries(start_time: str, pod: px.Pod):
    ''' Gets the network stats (transmitted/received traffic) for the input node.

    Args:
    @start_time Starting time of the data to examine.
    @node: The full name of the node to filter on.
    '''
    df = px.DataFrame(table='network_stats', start_time=start_time)
    df = df[df.ctx['pod'] == pod]
    df.timestamp = px.bin(df.time_, window_ns)

    # First calculate network usage by node over all windows.
    # Data is sharded by Pod in network_stats.
    df = df.groupby(['timestamp', 'pod_id']).agg(
        rx_bytes_end=('rx_bytes', px.max),
        rx_bytes_start=('rx_bytes', px.min),
        tx_bytes_end=('tx_bytes', px.max),
        tx_bytes_start=('tx_bytes', px.min),
        tx_errors_end=('tx_errors', px.max),
        tx_errors_start=('tx_errors', px.min),
        rx_errors_end=('rx_errors', px.max),
        rx_errors_start=('rx_errors', px.min),
        tx_drops_end=('tx_drops', px.max),
        tx_drops_start=('tx_drops', px.min),
        rx_drops_end=('rx_drops', px.max),
        rx_drops_start=('rx_drops', px.min),
    )

    # Calculate the network statistics rate over the window.
    # We subtract the counter value at the beginning ('_start')
    # from the value at the end ('_end').
    df.rx_bytes_per_ns = (df.rx_bytes_end - df.rx_bytes_start) / window_ns
    df.tx_bytes_per_ns = (df.tx_bytes_end - df.tx_bytes_start) / window_ns
    df.rx_drops_per_ns = (df.rx_drops_end - df.rx_drops_start) / window_ns
    df.tx_drops_per_ns = (df.tx_drops_end - df.tx_drops_start) / window_ns
    df.rx_errors_per_ns = (df.rx_errors_end - df.rx_errors_start) / window_ns
    df.tx_errors_per_ns = (df.tx_errors_end - df.tx_errors_start) / window_ns

    # Add up the network values per node.
    df = df.groupby(['timestamp']).agg(
        rx_bytes_per_ns=('rx_bytes_per_ns', px.sum),
        tx_bytes_per_ns=('tx_bytes_per_ns', px.sum),
        rx_drop_per_ns=('rx_drops_per_ns', px.sum),
        tx_drops_per_ns=('tx_drops_per_ns', px.sum),
        rx_errors_per_ns=('rx_errors_per_ns', px.sum),
        tx_errors_per_ns=('tx_errors_per_ns', px.sum),
    )
    df.time_ = df.timestamp
    return df


def inbound_latency_timeseries(start_time: str, pod: px.Pod):
    ''' Compute the latency as a timeseries for requests received by `pod`.

    Args:
    @start_time: The timestamp of data to start at.
    @pod: The name of the pod to filter on.

    '''
    df = filtered_http_events(start_time,
                              include_health_checks=not filter_health_checks,
                              include_ready_checks=not filter_ready_checks,
                              include_unresolved_inbound=not filter_unresolved_inbound)
    df.pod = df.ctx['pod']
    df = df[df.pod == pod]

    # Filter only to inbound pod traffic (server-side).
    # Don't include traffic initiated by this pod to an external location.
    df = df[df.trace_role == 2]

    df.timestamp = px.bin(df.time_, window_ns)
    df = df.groupby(['timestamp']).agg(
        latency_quantiles=('latency', px.quantiles)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df.latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p99')))
    df.time_ = df.timestamp
    return df[['time_', 'latency_p50', 'latency_p90', 'latency_p99']]


def inbound_request_timeseries_by_container(start_time: str, pod: px.Pod):
    ''' Compute the request statistics as a timeseries for requests received
        by `pod` by container.

    Args:
    @start_time: The timestamp of data to start at.
    @pod: The name of the pod to filter on.

    '''
    df = filtered_http_events(start_time,
                              include_health_checks=not filter_health_checks,
                              include_ready_checks=not filter_ready_checks,
                              include_unresolved_inbound=not filter_unresolved_inbound)
    df.pod = df.ctx['pod']
    df = df[df.pod == pod]

    # Filter only to inbound pod traffic (server-side).
    # Don't include traffic initiated by this pod to an external location.
    df = df[df.trace_role == 2]

    df.container = df.ctx['container']
    df.timestamp = px.bin(df.time_, window_ns)
    df = df.groupby(['timestamp', 'container']).agg(
        error_rate_per_window=('failure', px.mean),
        throughput_total=('latency', px.count)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df.request_throughput = df.throughput_total / window_ns
    df.errors_per_ns = df.error_rate_per_window * df.request_throughput / px.DurationNanos(1)
    df.error_rate = px.Percent(df.error_rate_per_window)
    df.time_ = df.timestamp

    return df[['time_', 'container', 'request_throughput', 'errors_per_ns', 'error_rate']]


def inbound_let_summary(start_time: str, pod: px.Pod):
    ''' Gets a summary of requests inbound to `pod`.

    Args:
    @start_time Starting time of the data to examine.
    @pod: The pod to filter on.
    '''
    df = filtered_http_events(start_time,
                              include_health_checks=not filter_health_checks,
                              include_ready_checks=not filter_ready_checks,
                              include_unresolved_inbound=not filter_unresolved_inbound)
    df.pod = df.ctx['pod']
    df = df[df.pod == pod]

    df = df.groupby(['remote_addr']).agg(
        latency=('latency', px.quantiles),
        total_request_count=('latency', px.count)
        requests_per_window=('time_', px.count),
        error_rate=('failure', px.mean),
    )

    df.error_rate = px.Percent(df.error_rate)

    # TODO(philkuz): Replace parse_duration with a more general function that accepts all time formats.
    df.start_time = px.now() + px.parse_duration(start_time)
    df.stop_time = px.now()
    df.window = df.stop_time - df.start_time
    df.request_throughput = df.requests_per_window / px.DurationNanos(df.window)

    df.requesting_ip = df.remote_addr
    df.requesting_pod_id = px.ip_to_pod_id(df.remote_addr)
    df.requesting_pod = px.pod_id_to_pod_name(df.requesting_pod_id)
    df.requesting_svc = px.pod_id_to_service_name(df.requesting_pod_id)

    return df[['requesting_ip', 'requesting_pod', 'requesting_svc', 'latency',
               'error_rate', 'request_throughput']]


def filtered_http_events(start_time: str, include_health_checks, include_ready_checks, include_unresolved_inbound):
    ''' Returns a dataframe of http events with health/ready requests filtered out and a
        failure field added.

    Args:
    @start_time: The timestamp of data to start at.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time)
    df.failure = df.resp_status >= 400

    filter_out_conds = ((df.req_path != '/healthz' or include_health_checks) and (
        df.req_path != '/readyz' or include_ready_checks)) and (
        df['remote_addr'] != '-' or include_unresolved_inbound)
    df = df[filter_out_conds]

    return df


def stacktraces(start_time: str, pod: str):
    df = px.DataFrame(table='stack_traces.beta', start_time=start_time)

    df.namespace = df.ctx['namespace']
    df.pod = df.ctx['pod']
    df.container = df.ctx['container']
    df.cmdline = df.ctx['cmdline']

    # Filter on the pod.
    df = df[df.pod == pod]

    # Get stack trace totals for the pod.
    # This must be done before any additional filtering to avoid skewing percentages.
    grouping_agg = df.groupby(["pod"]).agg(
        count=('count', px.sum)
    )

    # Combine flamegraphs from different intervals into one larger framegraph.
    df = df.groupby(['namespace', 'pod', 'container', 'cmdline', 'stack_trace_id']).agg(
        stack_trace=('stack_trace', px.any),
        count=('count', px.sum)
    )

    # Compute percentages.
    df = df.merge(
        grouping_agg,
        how='inner',
        left_on="pod",
        right_on="pod",
        suffixes=['', '_x']
    )
    df.percent = 100.0 * df.count / df.count_x
    return df.drop('pod_x')
