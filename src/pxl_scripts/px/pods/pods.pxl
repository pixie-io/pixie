'''Pods Overview

List of Pods monitored by Pixie in a given Namespace with their high level application metrics
(latency, error-rate & rps) and resource usage (cpu, writes, reads).

'''

import px


bytes_per_mb = 1024.0 * 1024.0
# Window size for computing timeseries
window_s = 10
# Flag to filter out requests that come from an unresolvable IP.
filter_unresolved_inbound = True
# Flag to filter out health checks from the data.
filter_health_checks = True
# Flag to filter out ready checks from the data.
filter_ready_checks = True


def pods(start_time: str, namespace: px.Namespace):
    ''' A list of pods in `namespace`.

    Args:
    @start_time: The timestamp of data to start at.
    @namespace: The name of the namespace to filter on.

    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df = df[df.ctx['namespace'] == namespace]
    df.pod = df.ctx['pod_name']
    df.container = df.ctx['container_name']
    df.service = df.ctx['service']
    df = df.groupby(['service', 'pod', 'container']).agg()
    df = df.groupby(['service', 'pod']).agg(containers=('container', px.count))
    df.start_time = px.pod_name_to_start_time(df.pod)
    df.status = px.pod_name_to_status(df.pod)
    return df[['pod', 'service', 'start_time', 'containers', 'status']]


def resource_timeseries(start_time: str, namespace: px.Namespace):
    ''' Compute the resource usage as a timeseries for pods in `namespace`.

    Args:
    @start_time: The timestamp of data to start at.
    @namespace: The name of the namespace to filter on.

    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df = df[df.ctx['namespace'] == namespace]
    df.pod = df.ctx['pod_name']
    df.timestamp = px.bin(df.time_, px.seconds(window_s))

    # Convert bytes to MB.
    df.vsize_mb = df.vsize_bytes / bytes_per_mb
    df.rss_mb = df.rss_bytes / bytes_per_mb
    df.read_mb = df.read_bytes / bytes_per_mb
    df.write_mb = df.write_bytes / bytes_per_mb
    df.rchar_mb = df.rchar_bytes / bytes_per_mb
    df.wchar_mb = df.wchar_bytes / bytes_per_mb

    # Convert nanoseconds to milliseconds.
    df.cpu_utime_ms = df.cpu_utime_ns / 1.0E6
    df.cpu_ktime_ms = df.cpu_ktime_ns / 1.0E6

    # First calculate CPU usage by process (UPID) in each k8s_object
    # over all windows.
    df = df.groupby(['upid', 'pod', 'timestamp']).agg(
        rss_mb=('rss_mb', px.mean),
        vsize_mb=('vsize_mb', px.mean),
        # The fields below are counters, so we take the min and the max to subtract them.
        cpu_utime_ms_max=('cpu_utime_ms', px.max),
        cpu_utime_ms_min=('cpu_utime_ms', px.min),
        cpu_ktime_ms_max=('cpu_ktime_ms', px.max),
        cpu_ktime_ms_min=('cpu_ktime_ms', px.min),
        read_mb_max=('read_mb', px.max),
        read_mb_min=('read_mb', px.min),
        write_mb_max=('write_mb', px.max),
        write_mb_min=('write_mb', px.min),
        rchar_mb_max=('rchar_mb', px.max),
        rchar_mb_min=('rchar_mb', px.min),
        wchar_mb_max=('wchar_mb', px.max),
        wchar_mb_min=('wchar_mb', px.min),
    )

    # Next calculate cpu usage and memory stats per window.
    df.cpu_utime_ms = df.cpu_utime_ms_max - df.cpu_utime_ms_min
    df.cpu_ktime_ms = df.cpu_ktime_ms_max - df.cpu_ktime_ms_min
    df.read_mb = df.read_mb_max - df.read_mb_min
    df.write_mb = df.write_mb_max - df.write_mb_min
    df.rchar_mb = df.rchar_mb_max - df.rchar_mb_min
    df.wchar_mb = df.wchar_mb_max - df.wchar_mb_min

    # Then aggregate process individual process metrics.
    df = df.groupby(['pod', 'timestamp']).agg(
        cpu_ktime_ms=('cpu_ktime_ms', px.sum),
        cpu_utime_ms=('cpu_utime_ms', px.sum),
        read_mb=('read_mb', px.sum),
        write_mb=('write_mb', px.sum),
        rchar_mb=('rchar_mb', px.sum),
        wchar_mb=('wchar_mb', px.sum),
        rss_mb=('rss_mb', px.sum),
        vsize_mb=('vsize_mb', px.sum),
    )

    # Convert window_s into the same units as cpu time.
    window_size_ms = window_s * 1.0E3
    # Finally, calculate total (kernel + user time)  percentage used over window.
    df.cpu_pct = (df.cpu_ktime_ms + df.cpu_utime_ms) / window_size_ms * 100
    df['time_'] = df['timestamp']
    return df.drop(['cpu_ktime_ms', 'cpu_utime_ms', 'timestamp'])


def inbound_let_timeseries(start_time: str, namespace: px.Namespace):
    ''' Compute the let as a timeseries for requests received by pods in `namespace`.

    Args:
    @start_time: The timestamp of data to start at.
    @namespace: The name of the namespace to filter on.

    '''
    df = inbound_let_helper(start_time, namespace)

    df = df.groupby(['pod', 'timestamp']).agg(
        latency_quantiles=('latency_ms', px.quantiles),
        error_rate_per_window=('failure', px.mean),
        throughput_total=('latency_ms', px.count),
        bytes_total=('resp_size', px.sum)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    window_size = window_s * 1.0
    df.latency_p50 = px.pluck_float64(df.latency_quantiles, 'p50')
    df.latency_p90 = px.pluck_float64(df.latency_quantiles, 'p90')
    df.latency_p99 = px.pluck_float64(df.latency_quantiles, 'p99')
    df.requests_per_s = df.throughput_total / window_size
    df.error_rate_pct = df.error_rate_per_window * 100
    df.bytes_per_s = df.bytes_total / window_size
    df.time_ = df.timestamp

    return df[['time_', 'pod', 'latency_p50', 'latency_p90', 'latency_p99',
               'requests_per_s', 'error_rate_pct', 'bytes_per_s']]


def inbound_let_summary(start_time: str, namespace: px.Namespace):
    ''' Gets a summary of request statistics for pods in `namespace`.

    Args:
    @start: Starting time of the data to examine.
    @namespace: The namespace to filter on.
    '''
    df = inbound_let_helper(start_time, namespace)

    quantiles_agg = df.groupby(['pod', 'remote_addr']).agg(
        latency_ms=('latency_ms', px.quantiles),
        total_request_count=('latency_ms', px.count)
    )
    quantiles_table = quantiles_agg[['pod', 'remote_addr', 'latency_ms',
                                     'total_request_count']]

    range_agg = df.groupby(['pod', 'remote_addr', 'timestamp']).agg(
        requests_per_window=('time_', px.count),
        error_rate=('failure', px.mean)
    )

    rps_table = range_agg.groupby(['pod', 'remote_addr']).agg(
        requests_per_window=('requests_per_window', px.mean),
        error_rate=('error_rate', px.mean)
    )

    joined_table = quantiles_table.merge(rps_table,
                                         how='inner',
                                         left_on=['pod', 'remote_addr'],
                                         right_on=['pod', 'remote_addr'],
                                         suffixes=['', '_x'])

    joined_table.error_rate_pct = 100 * joined_table.error_rate
    joined_table.requests_per_s = joined_table.requests_per_window / (1.0 * window_s)

    joined_table.responder = df.pod
    joined_table.requestor = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))

    return joined_table[['responder', 'requestor', 'latency_ms', 'error_rate_pct',
                         'requests_per_s']]


def inbound_let_helper(start_time: str, namespace: px.Namespace):
    ''' Compute the initial part of the let for requests received or sent by pods in `namespace`.

    Args:
    @start_time: The timestamp of data to start at.
    @namespace: The name of the namespace to filter on.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time)
    df = df[df.ctx['namespace'] == namespace]
    df.pod = df.ctx['pod']
    df = df[df.pod != '']
    df.latency_ms = df.http_resp_latency_ns / 1.0E6
    df = df[df['latency_ms'] < 10000.0]
    df.timestamp = px.bin(df.time_, px.seconds(window_s))

    df.resp_size = px.length(df.http_resp_body)
    df.failure = df.http_resp_status >= 400
    filter_out_conds = ((df.http_req_path != '/health' or not filter_health_checks) and (
        df.http_req_path != '/readyz' or not filter_ready_checks)) and (
        df['remote_addr'] != '-' or not filter_unresolved_inbound)

    df = df[filter_out_conds]
    return df
